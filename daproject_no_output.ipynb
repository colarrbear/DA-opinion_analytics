{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colarrbear/DA-opinion_analytics/blob/main/DA-opinion_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aspect-Based Sentiment Analysis (ABSA) for Sephora Product Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chfjf8wgszZB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import string\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load product info dataset\n",
        "product_df = pd.read_csv('datasets/product_info.csv')\n",
        "product_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load review datasets and concatenate\n",
        "review_files = [\n",
        "    'datasets/reviews_0-250.csv',\n",
        "    'datasets/reviews_250-500.csv',\n",
        "    'datasets/reviews_500-750.csv',\n",
        "    'datasets/reviews_750-1250.csv',\n",
        "    'datasets/reviews_1250-end.csv'\n",
        "]\n",
        "\n",
        "reviews_list = []\n",
        "for file in review_files:\n",
        "    df = pd.read_csv(file)\n",
        "    reviews_list.append(df)\n",
        "    \n",
        "if not reviews_list:\n",
        "    raise ValueError(\"No review data was loaded. Please check file paths.\")\n",
        "\n",
        "reviews = pd.concat(reviews_list, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic exploration\n",
        "print(f\"Product info shape: {product_df.shape}\")\n",
        "print(f\"Reviews shape: {reviews.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first few rows of each dataset\n",
        "product_df.head()\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values in product_info:\")\n",
        "product_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMissing values in reviews:\")\n",
        "reviews.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"\\nBasic statistics for numerical columns in product_info:\")\n",
        "product_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBasic statistics for numerical columns in reviews:\")\n",
        "reviews.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of ratings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(reviews['rating'], bins=5, kde=True)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of product categories\n",
        "plt.figure(figsize=(12, 8))\n",
        "primary_categories = product_df['primary_category'].value_counts().head(10)\n",
        "sns.barplot(x=primary_categories.values, y=primary_categories.index)\n",
        "plt.title('Top 10 Primary Product Categories')\n",
        "plt.xlabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge product information with reviews on product_id\n",
        "merged_data = pd.merge(reviews, product_df, on=['product_id', 'product_name', 'brand_name', 'price_usd'], how='inner')\n",
        "\n",
        "# Check shape after merging\n",
        "print(f\"Merged data shape: {merged_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to clean review text\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Clean review text\n",
        "merged_data['clean_review_text'] = merged_data['review_text'].apply(clean_text)\n",
        "merged_data['clean_review_title'] = merged_data['review_title'].apply(clean_text)\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return []\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "merged_data['tokens'] = merged_data['clean_review_text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# Create a column with joined tokens for further analysis\n",
        "merged_data['processed_text'] = merged_data['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Check the results\n",
        "merged_data[['review_text', 'clean_review_text', 'processed_text']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all column names\n",
        "print(repr(merged_data.columns.tolist()))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transformation and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sentiment score based on rating\n",
        "merged_data['sentiment_score'] = merged_data['rating_x'].apply(lambda x: 'positive' if x > 3 else 'negative' if x < 3 else 'neutral')\n",
        "\n",
        "# Convert categorical features to one-hot encoding\n",
        "categorical_cols = ['primary_category', 'secondary_category', 'tertiary_category', 'skin_type', 'eye_color', 'skin_tone', 'hair_color']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in merged_data.columns:\n",
        "        # Fill NaN values with 'unknown'\n",
        "        merged_data[col] = merged_data[col].fillna('unknown')\n",
        "        \n",
        "        # Create one-hot encoding\n",
        "        one_hot = pd.get_dummies(merged_data[col], prefix=col)\n",
        "        \n",
        "        # Join the encoded df\n",
        "        merged_data = merged_data.join(one_hot)\n",
        "\n",
        "# Extract year and month from submission_time for temporal analysis\n",
        "merged_data['submission_time'] = pd.to_datetime(merged_data['submission_time'])\n",
        "merged_data['review_year'] = merged_data['submission_time'].dt.year\n",
        "merged_data['review_month'] = merged_data['submission_time'].dt.month\n",
        "\n",
        "# Create a feature for review length\n",
        "merged_data['review_length'] = merged_data['review_text'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "# Create TF-IDF features for review text\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=5)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_data['processed_text'].fillna(''))\n",
        "\n",
        "# Convert TF-IDF matrix to dataframe\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the shape of TF-IDF dataframe\n",
        "print(f\"TF-IDF matrix shape: {tfidf_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspect Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define common aspects to look for in beauty product reviews\n",
        "beauty_aspects = {\n",
        "    'packaging': ['packaging', 'container', 'bottle', 'jar', 'tube', 'pump', 'applicator'],\n",
        "    'texture': ['texture', 'consistency', 'creamy', 'lightweight', 'heavy', 'sticky', 'oily', 'watery', 'silky', 'smooth'],\n",
        "    'scent': ['scent', 'fragrance', 'smell', 'odor', 'aroma', 'perfume'],\n",
        "    'efficacy': ['works', 'effective', 'result', 'results', 'difference', 'effective', 'ineffective', 'improvement'],\n",
        "    'ingredients': ['ingredient', 'ingredients', 'natural', 'chemical', 'organic', 'paraben', 'sulfate', 'alcohol', 'oil'],\n",
        "    'longevity': ['last', 'lasting', 'longevity', 'wear', 'duration', 'long-lasting', 'short-lived', 'wears off'],\n",
        "    'value': ['price', 'expensive', 'affordable', 'worth', 'value', 'cost', 'pricey', 'cheap', 'investment', 'bargain'],\n",
        "    'application': ['apply', 'application', 'absorb', 'absorption', 'blend', 'blending', 'spreads', 'rub'],\n",
        "    'color': ['color', 'pigment', 'shade', 'tint', 'tone', 'hue', 'pigmentation', 'vibrant', 'dull']\n",
        "}\n",
        "\n",
        "# Function to identify aspects in a review\n",
        "def extract_aspects(text, aspect_dict):\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return {}\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    found_aspects = {}\n",
        "    \n",
        "    for aspect, keywords in aspect_dict.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in text_lower:\n",
        "                if aspect not in found_aspects:\n",
        "                    found_aspects[aspect] = []\n",
        "                found_aspects[aspect].append(keyword)\n",
        "    \n",
        "    return found_aspects\n",
        "\n",
        "# Apply aspect extraction to each review\n",
        "merged_data['aspects'] = merged_data['clean_review_text'].apply(lambda x: extract_aspects(x, beauty_aspects))\n",
        "\n",
        "# Create binary flags for each aspect\n",
        "for aspect in beauty_aspects.keys():\n",
        "    merged_data[f'has_{aspect}'] = merged_data['aspects'].apply(lambda x: 1 if aspect in x else 0)\n",
        "\n",
        "# Calculate the count of each aspect\n",
        "aspect_counts = merged_data[[f'has_{aspect}' for aspect in beauty_aspects.keys()]].sum().reset_index()\n",
        "aspect_counts.columns = ['aspect', 'count']\n",
        "aspect_counts['aspect'] = aspect_counts['aspect'].apply(lambda x: x.replace('has_', ''))\n",
        "\n",
        "# Visualize aspect counts\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='count', y='aspect', data=aspect_counts.sort_values('count', ascending=False))\n",
        "plt.title('Frequency of Aspects Mentioned in Reviews')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Aspect')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment Analysis by Aspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 1: Calculate Review-Level VADER Scores (Needed by many sections) ---\n",
        "\n",
        "print(\"--- Calculating Review-Level VADER Scores ---\")\n",
        "# Download VADER lexicon and sentence tokenizer if needed\n",
        "try:\n",
        "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
        "except LookupError:\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "\n",
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment scores for text (used for both review and sentence level)\n",
        "def get_sentiment_scores(text):\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return {\"neg\": 0, \"neu\": 0, \"pos\": 0, \"compound\": 0}\n",
        "    # Limit text length for VADER if necessary (optional, VADER handles long text but can be slow)\n",
        "    # max_len = 3000\n",
        "    # if len(text) > max_len:\n",
        "    #     text = text[:max_len]\n",
        "    return sid.polarity_scores(text)\n",
        "\n",
        "# Apply sentiment analysis to the CLEANED review text\n",
        "print(\"Calculating VADER scores for each review (using clean_review_text)...\")\n",
        "if 'clean_review_text' not in merged_data.columns:\n",
        "     raise ValueError(\"Column 'clean_review_text' not found. Ensure preprocessing is complete.\")\n",
        "\n",
        "# It's generally better to apply VADER to text with punctuation, but clean_review_text is often used\n",
        "# If clean_review_text removed too much, consider applying to 'review_text' but handle potential errors\n",
        "merged_data['sentiment_scores'] = merged_data['clean_review_text'].apply(get_sentiment_scores)\n",
        "print(\"VADER scores calculated.\")\n",
        "\n",
        "# Extract sentiment scores into separate columns in merged_data\n",
        "print(\"Extracting scores into columns (neg_score, neu_score, pos_score, compound_score)...\")\n",
        "merged_data['neg_score'] = merged_data['sentiment_scores'].apply(lambda x: x.get('neg', 0))\n",
        "merged_data['neu_score'] = merged_data['sentiment_scores'].apply(lambda x: x.get('neu', 0))\n",
        "merged_data['pos_score'] = merged_data['sentiment_scores'].apply(lambda x: x.get('pos', 0))\n",
        "merged_data['compound_score'] = merged_data['sentiment_scores'].apply(lambda x: x.get('compound', 0)) # CRITICAL LINE\n",
        "print(\"'compound_score' column added to merged_data.\")\n",
        "\n",
        "# Verify column exists\n",
        "if 'compound_score' in merged_data.columns:\n",
        "    print(\"Verification successful: 'compound_score' is in merged_data.columns\")\n",
        "    print(f\"NaNs in compound_score: {merged_data['compound_score'].isnull().sum()}\")\n",
        "else:\n",
        "    print(\"Verification FAILED: 'compound_score' is NOT in merged_data.columns\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 2: Perform Sentence-Level Aspect Sentiment Analysis (Improved Granularity) ---\n",
        "\n",
        "print(\"--- Starting Sentence-Level Aspect Sentiment Analysis ---\")\n",
        "\n",
        "def analyze_aspect_sentiment_sentence_level(data, aspect_dict):\n",
        "    \"\"\"\n",
        "    Analyzes sentiment for aspects at the sentence level.\n",
        "    Uses 'review_text' for sentence tokenization and sentiment scoring.\n",
        "    Uses 'clean_review_text' for keyword matching.\n",
        "    \"\"\"\n",
        "    sentence_aspect_sentiments = []\n",
        "    processed_reviews = 0\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        original_text = row[\"review_text\"]\n",
        "        clean_text_for_match = row[\"clean_review_text\"] # Use clean text just for matching\n",
        "\n",
        "        if pd.isna(original_text) or not original_text:\n",
        "            continue\n",
        "        # No need to check clean_text_for_match for NaN if original_text is valid\n",
        "\n",
        "        try:\n",
        "            # Tokenize ORIGINAL text to preserve punctuation for VADER\n",
        "            sentences = sent_tokenize(original_text)\n",
        "        except Exception as e:\n",
        "            # print(f\"Warning: Error tokenizing review {index}: {e}. Using full text.\")\n",
        "            sentences = [original_text] # Fallback\n",
        "\n",
        "        # We need a way to match keywords in the cleaned text space\n",
        "        # Option 1: Tokenize clean text too (as before, risk of mismatch)\n",
        "        # Option 2: Clean each sentence individually for matching (safer but slower)\n",
        "\n",
        "        # Using Option 2 here for better robustness:\n",
        "        for sentence in sentences:\n",
        "            # Clean the current sentence specifically for keyword matching\n",
        "            current_clean_sentence_for_match = clean_text(sentence) # Apply your clean_text function\n",
        "\n",
        "            sentence_aspects_found = set() # Track aspects found in this sentence\n",
        "            for aspect, keywords in aspect_dict.items():\n",
        "                # Check the cleaned sentence for the keyword\n",
        "                if any(keyword in current_clean_sentence_for_match for keyword in keywords):\n",
        "                    sentence_aspects_found.add(aspect)\n",
        "\n",
        "            if sentence_aspects_found:\n",
        "                # Calculate sentiment for the ORIGINAL sentence (with punctuation)\n",
        "                sentence_sentiment = get_sentiment_scores(sentence)\n",
        "                for aspect in sentence_aspects_found:\n",
        "                    sentence_aspect_sentiments.append(\n",
        "                        {\n",
        "                            \"review_id\": index, # Or use actual review ID if available\n",
        "                            \"aspect\": aspect,\n",
        "                            \"sentence\": sentence, # Store original sentence\n",
        "                            \"compound\": sentence_sentiment[\"compound\"],\n",
        "                            \"neg\": sentence_sentiment[\"neg\"],\n",
        "                            \"neu\": sentence_sentiment[\"neu\"],\n",
        "                            \"pos\": sentence_sentiment[\"pos\"],\n",
        "                        }\n",
        "                    )\n",
        "        processed_reviews += 1\n",
        "        if processed_reviews % 10000 == 0:\n",
        "            print(f\"Processed {processed_reviews} reviews for sentence analysis...\")\n",
        "\n",
        "    print(f\"Finished processing {processed_reviews} reviews for sentence analysis.\")\n",
        "    if not sentence_aspect_sentiments:\n",
        "        print(\"Warning: No aspects found in any sentences during sentence-level analysis.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert list to DataFrame\n",
        "    sentence_df = pd.DataFrame(sentence_aspect_sentiments)\n",
        "\n",
        "    # Aggregate results by aspect\n",
        "    aspect_sentiment_agg = sentence_df.groupby(\"aspect\").agg(\n",
        "        avg_compound=(\"compound\", \"mean\"),\n",
        "        sentence_count=(\"sentence\", \"count\"),\n",
        "        positive_sentences=(\"compound\", lambda x: (x > 0.05).sum()),\n",
        "        neutral_sentences=(\"compound\", lambda x: ((x >= -0.05) & (x <= 0.05)).sum()),\n",
        "        negative_sentences=(\"compound\", lambda x: (x < -0.05).sum()),\n",
        "    ).reset_index() # Add reset_index here\n",
        "\n",
        "    # Calculate percentages\n",
        "    total_sentences = (\n",
        "        aspect_sentiment_agg[\"positive_sentences\"] +\n",
        "        aspect_sentiment_agg[\"neutral_sentences\"] +\n",
        "        aspect_sentiment_agg[\"negative_sentences\"]\n",
        "    )\n",
        "    # Avoid division by zero if total_sentences is 0\n",
        "    aspect_sentiment_agg[\"positive_pct\"] = np.where(\n",
        "        total_sentences > 0,\n",
        "        (aspect_sentiment_agg[\"positive_sentences\"] / total_sentences * 100),\n",
        "        0\n",
        "    )\n",
        "    aspect_sentiment_agg[\"neutral_pct\"] = np.where(\n",
        "        total_sentences > 0,\n",
        "        (aspect_sentiment_agg[\"neutral_sentences\"] / total_sentences * 100),\n",
        "        0\n",
        "    )\n",
        "    aspect_sentiment_agg[\"negative_pct\"] = np.where(\n",
        "        total_sentences > 0,\n",
        "        (aspect_sentiment_agg[\"negative_sentences\"] / total_sentences * 100),\n",
        "        0\n",
        "    )\n",
        "\n",
        "    return aspect_sentiment_agg # Return the aggregated DataFrame\n",
        "\n",
        "\n",
        "# --- Apply the sentence-level function ---\n",
        "aspect_sentiment_df_sentence = analyze_aspect_sentiment_sentence_level(\n",
        "    merged_data, beauty_aspects\n",
        ")\n",
        "print(\"Sentence-level analysis complete.\")\n",
        "print(\"Aggregated Sentence-Level Sentiment Head:\")\n",
        "print(aspect_sentiment_df_sentence.head())\n",
        "\n",
        "\n",
        "# --- Visualize the sentence-level results ---\n",
        "if not aspect_sentiment_df_sentence.empty:\n",
        "    print(\"\\n--- Visualizing Sentence-Level Sentiment Distribution ---\")\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    # Ensure columns exist before plotting\n",
        "    plot_cols = ['aspect', 'positive_pct', 'neutral_pct', 'negative_pct']\n",
        "    if all(col in aspect_sentiment_df_sentence.columns for col in plot_cols):\n",
        "        x = aspect_sentiment_df_sentence[\"aspect\"]\n",
        "        y1 = aspect_sentiment_df_sentence[\"positive_pct\"]\n",
        "        y2 = aspect_sentiment_df_sentence[\"neutral_pct\"]\n",
        "        y3 = aspect_sentiment_df_sentence[\"negative_pct\"]\n",
        "\n",
        "        width = 0.25\n",
        "        x_pos = np.arange(len(x))\n",
        "\n",
        "        plt.bar(x_pos - width, y1, width=width, color=\"green\", label=\"Positive Sentences\")\n",
        "        plt.bar(x_pos, y2, width=width, color=\"gray\", label=\"Neutral Sentences\")\n",
        "        plt.bar(x_pos + width, y3, width=width, color=\"red\", label=\"Negative Sentences\")\n",
        "\n",
        "        plt.xlabel(\"Aspect\")\n",
        "        plt.ylabel(\"Percentage of Sentences\")\n",
        "        plt.title(\"Sentence Sentiment Distribution Across Different Product Aspects\")\n",
        "        plt.xticks(x_pos, x, rotation=45, ha=\"right\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not plot sentence distribution: Missing required columns.\")\n",
        "\n",
        "    # Optional: Visualize average compound score per aspect (sentence-based)\n",
        "    print(\"\\n--- Visualizing Average Sentence Sentiment Score ---\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    if 'avg_compound' in aspect_sentiment_df_sentence.columns and 'aspect' in aspect_sentiment_df_sentence.columns:\n",
        "        sns.barplot(\n",
        "            x=\"avg_compound\",\n",
        "            y=\"aspect\",\n",
        "            data=aspect_sentiment_df_sentence.sort_values(\"avg_compound\", ascending=False),\n",
        "            palette=\"RdYlGn\",\n",
        "        )\n",
        "        plt.title(\"Average Sentence Sentiment Score by Aspect\")\n",
        "        plt.xlabel(\"Average Compound Score (Sentence Level)\")\n",
        "        plt.axvline(x=0, color=\"grey\", linestyle=\"--\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "         print(\"Could not plot average sentence score: Missing required columns.\")\n",
        "\n",
        "else:\n",
        "    print(\"No aspect sentiment data from sentence-level analysis to visualize.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced ABSA with Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Requires sentence tokenization and aspect extraction to be done first)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize # Make sure it's imported\n",
        "\n",
        "# --- Function with added check ---\n",
        "def create_aspect_sentence_dataset(data, aspect):\n",
        "    \"\"\"Creates dataset using only sentences containing the aspect.\"\"\"\n",
        "\n",
        "    # --- ADD CHECK HERE ---\n",
        "    required_cols = ['review_text', 'clean_review_text', 'compound_score', f'has_{aspect}']\n",
        "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"The input DataFrame 'data' is missing required columns: {missing_cols}. \"\n",
        "                         \"Please ensure preprocessing and VADER sentiment analysis are complete.\")\n",
        "    # --- END CHECK ---\n",
        "\n",
        "    # Filter to reviews mentioning this aspect\n",
        "    aspect_reviews = data[data[f'has_{aspect}'] == 1].copy()\n",
        "\n",
        "    if aspect_reviews.shape[0] < 50: # Initial check based on reviews mentioning aspect\n",
        "        # print(f\"Skipping aspect '{aspect}': Less than 50 reviews mention it.\")\n",
        "        return None, None # Return None for both X and y\n",
        "\n",
        "    sentences_with_aspect = []\n",
        "    review_indices = [] # Keep track of original review index for sentiment label\n",
        "\n",
        "    # Define keywords for the specific aspect\n",
        "    aspect_keywords = beauty_aspects.get(aspect, [])\n",
        "    if not aspect_keywords:\n",
        "        print(f\"Warning: No keywords defined for aspect '{aspect}'.\")\n",
        "        return None, None\n",
        "\n",
        "    for index, row in aspect_reviews.iterrows():\n",
        "        text = row[\"review_text\"] # Use original for sentence tokenization\n",
        "        # clean_text_full = row[\"clean_review_text\"] # Use clean for processing later\n",
        "\n",
        "        if pd.isna(text): continue\n",
        "\n",
        "        try:\n",
        "            # Use NLTK's sentence tokenizer\n",
        "            sentences = sent_tokenize(text)\n",
        "        except Exception as e:\n",
        "            # print(f\"Warning: Could not sentence tokenize review index {index}. Using full text. Error: {e}\")\n",
        "            sentences = [text] # Fallback\n",
        "\n",
        "        relevant_sentences_text = []\n",
        "        for sentence in sentences:\n",
        "            # Check the lowercase sentence for keywords\n",
        "            # Simple check, might need refinement (e.g., word boundaries)\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in aspect_keywords):\n",
        "                 # Clean and process *only this sentence*\n",
        "                 cleaned_sentence = clean_text(sentence) # Apply your clean_text function\n",
        "                 tokens = tokenize_and_lemmatize(cleaned_sentence) # Apply your tokenizer\n",
        "                 processed_sentence = ' '.join(tokens)\n",
        "                 relevant_sentences_text.append(processed_sentence)\n",
        "\n",
        "        if relevant_sentences_text:\n",
        "            # Join all relevant processed sentences for this review\n",
        "            sentences_with_aspect.append(\" \".join(relevant_sentences_text))\n",
        "            review_indices.append(index) # Store index to get sentiment label later\n",
        "\n",
        "    if len(sentences_with_aspect) < 40: # Check after collecting sentences\n",
        "        # print(f\"Skipping aspect '{aspect}': Less than 40 reviews with relevant sentences found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Get the corresponding sentiment labels from the original data using stored indices\n",
        "    # Using compound score derived label (still a proxy, but features are more focused)\n",
        "    sentiment_labels = data.loc[review_indices, 'compound_score'].apply(\n",
        "        lambda x: 1 if x > 0.05 else 0 if x < -0.05 else -1 # 1=pos, 0=neg, -1=neu\n",
        "    )\n",
        "\n",
        "    # Filter out neutral if needed for binary classification\n",
        "    valid_indices = sentiment_labels != -1\n",
        "    # Use list comprehension for filtering based on boolean mask\n",
        "    final_sentences = [sentences_with_aspect[i] for i, valid in enumerate(valid_indices) if valid]\n",
        "    final_labels = sentiment_labels[valid_indices]\n",
        "\n",
        "    if len(final_sentences) < 30: # Final check for binary classification data\n",
        "         # print(f\"Skipping aspect '{aspect}': Less than 30 non-neutral examples.\")\n",
        "         return None, None\n",
        "\n",
        "    return final_sentences, final_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- (Rest of ML training loop follows) ---\n",
        "# Make sure beauty_aspects, clean_text, tokenize_and_lemmatize are defined before this loop\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Checking merged_data columns BEFORE ML loop:\")\n",
        "if 'merged_data' in locals() and isinstance(merged_data, pd.DataFrame):\n",
        "    print(merged_data.columns)\n",
        "    if 'compound_score' in merged_data.columns:\n",
        "        print(\"'compound_score' IS present.\")\n",
        "        # Optional: Check for NaNs which might indicate issues\n",
        "        print(f\"NaNs in compound_score: {merged_data['compound_score'].isnull().sum()}\")\n",
        "    else:\n",
        "        print(\"ERROR: 'compound_score' IS NOT present before the loop!\")\n",
        "else:\n",
        "    print(\"ERROR: merged_data DataFrame not found or not a DataFrame.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Your existing ML loop starts here ---\n",
        "aspect_models = {}\n",
        "aspect_performances = {}\n",
        "# ... rest of the loop ...\n",
        "\n",
        "\n",
        "# Example call structure (ensure merged_data is ready):\n",
        "# aspect_models = {}\n",
        "# aspect_performances = {}\n",
        "# for aspect in beauty_aspects.keys():\n",
        "#     print(f\"\\nTraining classifier for aspect: {aspect}\")\n",
        "#     X_text, y = create_aspect_sentence_dataset(merged_data, aspect)\n",
        "#     if X_text is None:\n",
        "#          print(f\"Insufficient data for aspect '{aspect}', skipping.\")\n",
        "#          continue\n",
        "#     # ... rest of training ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train aspect-specific sentiment classifiers\n",
        "aspect_models = {}\n",
        "aspect_performances = {}\n",
        "\n",
        "for aspect in beauty_aspects.keys():\n",
        "    print(f\"\\nTraining classifier for aspect: {aspect}\")\n",
        "    \n",
        "    # Create dataset for this aspect\n",
        "    aspect_data = create_aspect_dataset(merged_data, aspect)\n",
        "    \n",
        "    if aspect_data is None:\n",
        "        print(f\"Insufficient data for aspect '{aspect}', skipping.\")\n",
        "        continue\n",
        "    \n",
        "    # Prepare features and target\n",
        "    X = tfidf_vectorizer.transform(aspect_data['processed_text'].fillna(''))\n",
        "    y = aspect_data['sentiment_label']\n",
        "    \n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    \n",
        "    aspect_models[aspect] = model\n",
        "    aspect_performances[aspect] = {\n",
        "        'accuracy': accuracy,\n",
        "        'report': report,\n",
        "        'confusion_matrix': pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy for {aspect}: {accuracy:.4f}\")\n",
        "    print(report)\n",
        "\n",
        "# Create a function to analyze a new review with aspect-based sentiment\n",
        "def analyze_review_aspects(review_text, aspect_models, tfidf_vectorizer, beauty_aspects):\n",
        "    # Clean and process the text\n",
        "    clean_text_val = clean_text(review_text)\n",
        "    tokens = tokenize_and_lemmatize(clean_text_val)\n",
        "    processed_text = ' '.join(tokens)\n",
        "    \n",
        "    # Extract aspects mentioned\n",
        "    aspects_mentioned = extract_aspects(clean_text_val, beauty_aspects)\n",
        "    \n",
        "    # Get VADER sentiment\n",
        "    vader_sentiment = get_sentiment_scores(clean_text_val)\n",
        "    \n",
        "    # Transform text to TF-IDF\n",
        "    text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
        "    \n",
        "    # Get aspect-specific sentiments\n",
        "    aspect_sentiments = {}\n",
        "    \n",
        "    for aspect, _ in aspects_mentioned.items():\n",
        "        if aspect in aspect_models:\n",
        "            # Predict sentiment with the aspect-specific model\n",
        "            sentiment_pred = aspect_models[aspect].predict(text_tfidf)[0]\n",
        "            sentiment_label = \"positive\" if sentiment_pred == 1 else \"negative\"\n",
        "            \n",
        "            # Get probability\n",
        "            sentiment_prob = max(aspect_models[aspect].predict_proba(text_tfidf)[0])\n",
        "            \n",
        "            aspect_sentiments[aspect] = {\n",
        "                'sentiment': sentiment_label,\n",
        "                'confidence': sentiment_prob\n",
        "            }\n",
        "    \n",
        "    return {\n",
        "        'aspects_mentioned': aspects_mentioned,\n",
        "        'overall_sentiment': vader_sentiment,\n",
        "        'aspect_sentiments': aspect_sentiments\n",
        "    }\n",
        "\n",
        "# Test the function with a sample review\n",
        "sample_review = \"I love the texture of this foundation. It goes on so smoothly and gives a nice dewy finish. \" \\\n",
        "                \"The packaging is beautiful but a bit bulky for travel. The scent is strong but not unpleasant. \" \\\n",
        "                \"I think it's a bit expensive for the amount you get, but it lasts a long time.\"\n",
        "\n",
        "analysis = analyze_review_aspects(sample_review, aspect_models, tfidf_vectorizer, beauty_aspects)\n",
        "print(\"\\nSample Review ABSA:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "print(json.dumps(analysis, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization and Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Create a function to visualize the most common terms for each aspect with sentiment\n",
        "def visualize_aspect_terms(data, aspect, sentiment='positive', top_n=15):\n",
        "    # Filter reviews that mention this aspect and have the specified sentiment\n",
        "    if sentiment == 'positive':\n",
        "        filtered_data = data[(data[f'has_{aspect}'] == 1) & (data['compound_score'] > 0.05)]\n",
        "    elif sentiment == 'negative':\n",
        "        filtered_data = data[(data[f'has_{aspect}'] == 1) & (data['compound_score'] < -0.05)]\n",
        "    else:\n",
        "        filtered_data = data[(data[f'has_{aspect}'] == 1) & \n",
        "                             (data['compound_score'] >= -0.05) & \n",
        "                             (data['compound_score'] <= 0.05)]\n",
        "    \n",
        "    # Combine all processed text\n",
        "    all_text = ' '.join(filtered_data['processed_text'].fillna(''))\n",
        "    \n",
        "    # Count term frequencies\n",
        "    vectorizer = CountVectorizer(max_features=top_n)\n",
        "    term_counts = vectorizer.fit_transform([all_text])\n",
        "    \n",
        "    # Get the terms and their counts\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    counts = term_counts.toarray()[0]\n",
        "    \n",
        "    # Create a dataframe for visualization\n",
        "    term_df = pd.DataFrame({'term': terms, 'count': counts})\n",
        "    term_df = term_df.sort_values('count', ascending=False)\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='count', y='term', data=term_df)\n",
        "    plt.title(f'Top {top_n} Terms in {sentiment.capitalize()} Reviews about {aspect.capitalize()}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize top terms for each aspect with positive and negative sentiment\n",
        "for aspect in beauty_aspects.keys():\n",
        "    print(f\"\\nAnalyzing {aspect}...\")\n",
        "    visualize_aspect_terms(merged_data, aspect, 'positive')\n",
        "    visualize_aspect_terms(merged_data, aspect, 'negative')\n",
        "\n",
        "# Create a brand-aspect sentiment matrix\n",
        "def create_brand_aspect_sentiment(data, brands, aspects):\n",
        "    brand_aspect_sentiment = []\n",
        "    \n",
        "    for brand in brands:\n",
        "        brand_data = data[data['brand_name'] == brand]\n",
        "        \n",
        "        for aspect in aspects:\n",
        "            # Filter reviews that mention this aspect for this brand\n",
        "            aspect_reviews = brand_data[brand_data[f'has_{aspect}'] == 1]\n",
        "            \n",
        "            if aspect_reviews.shape[0] < 5:  # Skip if too few reviews\n",
        "                continue\n",
        "            \n",
        "            # Calculate sentiment metrics\n",
        "            avg_rating = aspect_reviews['rating_x'].mean()\n",
        "            avg_compound = aspect_reviews['compound_score'].mean()\n",
        "            \n",
        "            pos_count = sum((aspect_reviews['compound_score'] > 0.05))\n",
        "            neu_count = sum((aspect_reviews['compound_score'] >= -0.05) & (aspect_reviews['compound_score'] <= 0.05))\n",
        "            neg_count = sum((aspect_reviews['compound_score'] < -0.05))\n",
        "            \n",
        "            total_count = pos_count + neu_count + neg_count\n",
        "            \n",
        "            if total_count > 0:\n",
        "                brand_aspect_sentiment.append({\n",
        "                    'brand': brand,\n",
        "                    'aspect': aspect,\n",
        "                    'avg_rating': avg_rating,\n",
        "                    'avg_compound': avg_compound,\n",
        "                    'positive_pct': pos_count / total_count * 100,\n",
        "                    'neutral_pct': neu_count / total_count * 100,\n",
        "                    'negative_pct': neg_count / total_count * 100,\n",
        "                    'total_reviews': total_count\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(brand_aspect_sentiment)\n",
        "\n",
        "# Get top brands by review count\n",
        "top_brands = merged_data['brand_name'].value_counts().head(10).index.tolist()\n",
        "\n",
        "# Create the brand-aspect sentiment matrix\n",
        "brand_aspect_df = create_brand_aspect_sentiment(merged_data, top_brands, beauty_aspects.keys())\n",
        "\n",
        "# Create a heatmap of positive sentiment percentage by brand and aspect\n",
        "brand_aspect_pivot = brand_aspect_df.pivot_table(\n",
        "    index='brand', \n",
        "    columns='aspect', \n",
        "    values='positive_pct',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(brand_aspect_pivot, annot=True, cmap='YlGnBu', fmt='.1f')\n",
        "plt.title('Positive Sentiment Percentage by Brand and Aspect')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a function for category-based ABSA\n",
        "def analyze_category_aspects(data, category_col, aspects):\n",
        "    category_aspect_sentiment = []\n",
        "    \n",
        "    for category in data[category_col].unique():\n",
        "        if pd.isna(category):\n",
        "            continue\n",
        "            \n",
        "        category_data = data[data[category_col] == category]\n",
        "        \n",
        "        for aspect in aspects:\n",
        "            # Filter reviews that mention this aspect for this category\n",
        "            aspect_reviews = category_data[category_data[f'has_{aspect}'] == 1]\n",
        "            \n",
        "            if aspect_reviews.shape[0] < 5:  # Skip if too few reviews\n",
        "                continue\n",
        "            \n",
        "            # Calculate sentiment metrics\n",
        "            avg_rating = aspect_reviews['rating_x'].mean()\n",
        "            avg_compound = aspect_reviews['compound_score'].mean()\n",
        "            \n",
        "            pos_count = sum((aspect_reviews['compound_score'] > 0.05))\n",
        "            neu_count = sum((aspect_reviews['compound_score'] >= -0.05) & (aspect_reviews['compound_score'] <= 0.05))\n",
        "            neg_count = sum((aspect_reviews['compound_score'] < -0.05))\n",
        "            \n",
        "            total_count = pos_count + neu_count + neg_count\n",
        "            \n",
        "            if total_count > 0:\n",
        "                category_aspect_sentiment.append({\n",
        "                    'category': category,\n",
        "                    'aspect': aspect,\n",
        "                    'avg_rating': avg_rating,\n",
        "                    'avg_compound': avg_compound,\n",
        "                    'positive_pct': pos_count / total_count * 100,\n",
        "                    'neutral_pct': neu_count / total_count * 100,\n",
        "                    'negative_pct': neg_count / total_count * 100,\n",
        "                    'total_reviews': total_count\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(category_aspect_sentiment)\n",
        "\n",
        "# Analyze primary category aspects\n",
        "category_aspect_df = analyze_category_aspects(merged_data, 'primary_category', beauty_aspects.keys())\n",
        "\n",
        "# Create a heatmap of positive sentiment percentage by category and aspect\n",
        "category_aspect_pivot = category_aspect_df.pivot_table(\n",
        "    index='category', \n",
        "    columns='aspect', \n",
        "    values='positive_pct',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(category_aspect_pivot, annot=True, cmap='YlGnBu', fmt='.1f')\n",
        "plt.title('Positive Sentiment Percentage by Category and Aspect')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Applications and Dashboard Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to find similar products based on aspect sentiments\n",
        "def find_similar_products(data, product_id, top_n=5):\n",
        "    \"\"\"Find products with similar aspect sentiment profiles\"\"\"\n",
        "    \n",
        "    # Get the aspect sentiment profile for the target product\n",
        "    target_product = data[data['product_id'] == product_id]\n",
        "    \n",
        "    if target_product.shape[0] == 0:\n",
        "        return \"Product not found\"\n",
        "    \n",
        "    target_name = target_product['product_name'].iloc[0]\n",
        "    target_brand = target_product['brand_name'].iloc[0]\n",
        "    \n",
        "    # Calculate product-aspect sentiment profiles for all products\n",
        "    products = []\n",
        "    \n",
        "    for pid in data['product_id'].unique():\n",
        "        product_data = data[data['product_id'] == pid]\n",
        "        \n",
        "        if product_data.shape[0] < 5:  # Skip products with few reviews\n",
        "            continue\n",
        "            \n",
        "        product_info = {\n",
        "            'product_id': pid,\n",
        "            'product_name': product_data['product_name'].iloc[0],\n",
        "            'brand_name': product_data['brand_name'].iloc[0],\n",
        "            'avg_rating': product_data['rating_x'].mean(),\n",
        "            'review_count': product_data.shape[0]\n",
        "        }\n",
        "        \n",
        "        # Add aspect sentiment scores\n",
        "        for aspect in beauty_aspects.keys():\n",
        "            aspect_reviews = product_data[product_data[f'has_{aspect}'] == 1]\n",
        "            \n",
        "            if aspect_reviews.shape[0] > 0:\n",
        "                product_info[f'{aspect}_sentiment'] = aspect_reviews['compound_score'].mean()\n",
        "            else:\n",
        "                product_info[f'{aspect}_sentiment'] = 0\n",
        "                \n",
        "        products.append(product_info)\n",
        "    \n",
        "    products_df = pd.DataFrame(products)\n",
        "    \n",
        "    # Get the target product data\n",
        "    target_profile = products_df[products_df['product_id'] == product_id]\n",
        "    \n",
        "    if target_profile.shape[0] == 0:\n",
        "        return \"Not enough reviews for similarity calculation\"\n",
        "    \n",
        "    # Remove the target product\n",
        "    products_df = products_df[products_df['product_id'] != product_id]\n",
        "    \n",
        "    # Calculate similarity based on aspect sentiment scores\n",
        "    aspect_cols = [f'{aspect}_sentiment' for aspect in beauty_aspects.keys()]\n",
        "    \n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    \n",
        "    target_features = target_profile[aspect_cols].values\n",
        "    other_features = products_df[aspect_cols].values\n",
        "    \n",
        "    similarities = cosine_similarity(target_features, other_features)[0]\n",
        "    \n",
        "    # Add similarity to the dataframe\n",
        "    products_df['similarity'] = similarities\n",
        "    \n",
        "    # Return top N similar products\n",
        "    similar_products = products_df.sort_values('similarity', ascending=False).head(top_n)\n",
        "    \n",
        "    return {\n",
        "        'target_product': {\n",
        "            'product_id': product_id,\n",
        "            'product_name': target_name,\n",
        "            'brand_name': target_brand\n",
        "        },\n",
        "        'similar_products': similar_products[['product_id', 'product_name', 'brand_name', 'avg_rating', 'similarity']].to_dict('records')\n",
        "    }\n",
        "\n",
        "# Test the similar products function with a sample product\n",
        "sample_product_id = merged_data['product_id'].iloc[0]\n",
        "similar_products = find_similar_products(merged_data, sample_product_id)\n",
        "print(\"\\nSimilar Products:\")\n",
        "print(json.dumps(similar_products, indent=4))\n",
        "\n",
        "# Create a function for product recommendation based on aspect preferences\n",
        "def recommend_products(data, aspect_preferences, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend products based on aspect preferences\n",
        "    \n",
        "    aspect_preferences: dict with aspect names as keys and importance scores (0-10) as values\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize aspect preferences\n",
        "    total_importance = sum(aspect_preferences.values())\n",
        "    normalized_preferences = {k: v/total_importance for k, v in aspect_preferences.items() if v > 0}\n",
        "    \n",
        "    # Calculate product-aspect sentiment profiles for all products\n",
        "    products = []\n",
        "    \n",
        "    for pid in data['product_id'].unique():\n",
        "        product_data = data[data['product_id'] == pid]\n",
        "        \n",
        "        if product_data.shape[0] < 5:  # Skip products with few reviews\n",
        "            continue\n",
        "            \n",
        "        product_info = {\n",
        "            'product_id': pid,\n",
        "            'product_name': product_data['product_name'].iloc[0],\n",
        "            'brand_name': product_data['brand_name'].iloc[0],\n",
        "            'primary_category': product_data['primary_category'].iloc[0] if 'primary_category' in product_data.columns else 'Unknown',\n",
        "            'avg_rating': product_data['rating_x'].mean(),\n",
        "            'review_count': product_data.shape[0]\n",
        "        }\n",
        "        \n",
        "        # Add aspect sentiment scores\n",
        "        aspect_score = 0\n",
        "        \n",
        "        for aspect, importance in normalized_preferences.items():\n",
        "            aspect_reviews = product_data[product_data[f'has_{aspect}'] == 1]\n",
        "            \n",
        "            if aspect_reviews.shape[0] > 0:\n",
        "                sentiment = aspect_reviews['compound_score'].mean()\n",
        "                # Convert to a 0-1 scale (from -1 to 1)\n",
        "                normalized_sentiment = (sentiment + 1) / 2\n",
        "                aspect_score += normalized_sentiment * importance\n",
        "                product_info[f'{aspect}_sentiment'] = sentiment\n",
        "            else:\n",
        "                product_info[f'{aspect}_sentiment'] = 0\n",
        "        \n",
        "        product_info['aspect_score'] = aspect_score\n",
        "        products.append(product_info)\n",
        "    \n",
        "    products_df = pd.DataFrame(products)\n",
        "    \n",
        "    # Return top N products by aspect score\n",
        "    recommended_products = products_df.sort_values('aspect_score', ascending=False).head(top_n)\n",
        "    \n",
        "    return recommended_products[['product_id', 'product_name', 'brand_name', 'primary_category', 'avg_rating', 'review_count', 'aspect_score'] + \n",
        "                               [f'{aspect}_sentiment' for aspect in normalized_preferences.keys()]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the recommendation function with sample preferences\n",
        "sample_preferences = {\n",
        "    'efficacy': 8,\n",
        "    'texture': 6,\n",
        "    'value': 5,\n",
        "    'packaging': 2,\n",
        "    'scent': 3\n",
        "}\n",
        "\n",
        "recommendations = recommend_products(merged_data, sample_preferences)\n",
        "print(\"\\nRecommended Products Based on Aspect Preferences:\")\n",
        "print(recommendations)\n",
        "\n",
        "# Create a function to analyze aspect sentiment trends over time\n",
        "def analyze_aspect_trends(data, aspect, time_unit='month'):\n",
        "    \"\"\"Analyze how sentiment for a specific aspect changes over time\"\"\"\n",
        "    \n",
        "    # Ensure we have datetime\n",
        "    data['submission_time'] = pd.to_datetime(data['submission_time'])\n",
        "    \n",
        "    # Filter reviews that mention this aspect\n",
        "    aspect_data = data[data[f'has_{aspect}'] == 1].copy()\n",
        "    \n",
        "    if aspect_data.shape[0] < 10:\n",
        "        return f\"Not enough data for aspect: {aspect}\"\n",
        "    \n",
        "    # Group by time unit\n",
        "    if time_unit == 'month':\n",
        "        aspect_data['time_period'] = aspect_data['submission_time'].dt.to_period('M')\n",
        "    elif time_unit == 'quarter':\n",
        "        aspect_data['time_period'] = aspect_data['submission_time'].dt.to_period('Q')\n",
        "    elif time_unit == 'year':\n",
        "        aspect_data['time_period'] = aspect_data['submission_time'].dt.to_period('Y')\n",
        "    else:\n",
        "        return \"Invalid time unit. Use 'month', 'quarter', or 'year'.\"\n",
        "    \n",
        "    # Calculate average sentiment by time period\n",
        "    sentiment_trends = aspect_data.groupby('time_period').agg({\n",
        "        'compound_score': 'mean',\n",
        "        'rating_x': 'mean',\n",
        "        'product_id': 'count'\n",
        "    }).reset_index()\n",
        "    \n",
        "    sentiment_trends.columns = ['time_period', 'avg_sentiment', 'avg_rating', 'review_count']\n",
        "    sentiment_trends['time_period'] = sentiment_trends['time_period'].astype(str)\n",
        "    \n",
        "    # Plot the trends\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "    \n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Time Period')\n",
        "    ax1.set_ylabel('Average Sentiment', color=color)\n",
        "    ax1.plot(sentiment_trends['time_period'], sentiment_trends['avg_sentiment'], color=color, marker='o')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.set_xticklabels(sentiment_trends['time_period'], rotation=45)\n",
        "    \n",
        "    # Create a second y-axis for review count\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Review Count', color=color)\n",
        "    ax2.bar(sentiment_trends['time_period'], sentiment_trends['review_count'], color=color, alpha=0.3)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    plt.title(f'Sentiment Trend for {aspect.capitalize()} Over Time')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return sentiment_trends\n",
        "\n",
        "# Test the trend analysis function for a sample aspect\n",
        "sample_trend = analyze_aspect_trends(merged_data, 'texture', 'month')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Topic Model for Deeper Aspect Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll use Latent Dirichlet Allocation (LDA) to find topics in reviews\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create document-term matrix\n",
        "count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
        "doc_term_matrix = count_vect.fit_transform(merged_data['processed_text'].fillna(''))\n",
        "\n",
        "# Create and fit the LDA model\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# Function to display the top words for each topic\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topics = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        topics[f\"Topic {topic_idx+1}\"] = topic_words\n",
        "        print(f\"Topic {topic_idx+1}: {' '.join(topic_words)}\")\n",
        "    return topics\n",
        "\n",
        "# Display the top 10 words for each topic\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "topics = display_topics(lda, feature_names, 10)\n",
        "\n",
        "# Transform the document-term matrix to get topic distributions for each review\n",
        "topic_distributions = lda.transform(doc_term_matrix)\n",
        "\n",
        "# Add the dominant topic to each review\n",
        "merged_data['dominant_topic'] = topic_distributions.argmax(axis=1) + 1\n",
        "\n",
        "# Calculate aspect sentiment by topic\n",
        "topic_aspect_sentiment = []\n",
        "\n",
        "for topic_num in range(1, lda.n_components + 1):\n",
        "    topic_reviews = merged_data[merged_data['dominant_topic'] == topic_num]\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        # Filter reviews that mention this aspect for this topic\n",
        "        aspect_reviews = topic_reviews[topic_reviews[f'has_{aspect}'] == 1]\n",
        "        \n",
        "        if aspect_reviews.shape[0] < 5:  # Skip if too few reviews\n",
        "            continue\n",
        "        \n",
        "        # Calculate sentiment metrics\n",
        "        avg_rating = aspect_reviews['rating_x'].mean()\n",
        "        avg_compound = aspect_reviews['compound_score'].mean()\n",
        "        \n",
        "        pos_count = sum((aspect_reviews['compound_score'] > 0.05))\n",
        "        neu_count = sum((aspect_reviews['compound_score'] >= -0.05) & (aspect_reviews['compound_score'] <= 0.05))\n",
        "        neg_count = sum((aspect_reviews['compound_score'] < -0.05))\n",
        "        \n",
        "        total_count = pos_count + neu_count + neg_count\n",
        "        \n",
        "        if total_count > 0:\n",
        "            topic_aspect_sentiment.append({\n",
        "                'topic': f\"Topic {topic_num}\",\n",
        "                'topic_words': ' '.join(topics[f\"Topic {topic_num}\"]),\n",
        "                'aspect': aspect,\n",
        "                'avg_rating': avg_rating,\n",
        "                'avg_compound': avg_compound,\n",
        "                'positive_pct': pos_count / total_count * 100,\n",
        "                'neutral_pct': neu_count / total_count * 100,\n",
        "                'negative_pct': neg_count / total_count * 100,\n",
        "                'total_reviews': total_count\n",
        "            })\n",
        "\n",
        "topic_aspect_df = pd.DataFrame(topic_aspect_sentiment)\n",
        "\n",
        "# Create a heatmap of positive sentiment percentage by topic and aspect\n",
        "topic_aspect_pivot = topic_aspect_df.pivot_table(\n",
        "    index='topic', \n",
        "    columns='aspect', \n",
        "    values='positive_pct',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(topic_aspect_pivot, annot=True, cmap='YlGnBu', fmt='.1f')\n",
        "plt.title('Positive Sentiment Percentage by Topic and Aspect')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Product Review Analyzer Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to analyze a single product in depth\n",
        "def analyze_product(data, product_id):\n",
        "    \"\"\"Comprehensive analysis of a single product\"\"\"\n",
        "    \n",
        "    # Filter data for this product\n",
        "    product_data = data[data['product_id'] == product_id].copy()\n",
        "    \n",
        "    if product_data.shape[0] == 0:\n",
        "        return \"Product not found\"\n",
        "    \n",
        "    # Basic product information\n",
        "    product_info = {\n",
        "        'product_id': product_id,\n",
        "        'product_name': product_data['product_name'].iloc[0],\n",
        "        'brand_name': product_data['brand_name'].iloc[0],\n",
        "        'primary_category': product_data['primary_category'].iloc[0] if 'primary_category' in product_data.columns else 'Unknown',\n",
        "        'price_usd': product_data['price_usd'].iloc[0],\n",
        "        'review_count': product_data.shape[0],\n",
        "        'avg_rating': product_data['rating_x'].mean(),\n",
        "        'avg_sentiment': product_data['compound_score'].mean()\n",
        "    }\n",
        "    \n",
        "    # Aspect analysis\n",
        "    aspect_analysis = []\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        # Filter reviews that mention this aspect\n",
        "        aspect_reviews = product_data[product_data[f'has_{aspect}'] == 1]\n",
        "        \n",
        "        if aspect_reviews.shape[0] == 0:\n",
        "            continue\n",
        "        \n",
        "        # Calculate sentiment metrics\n",
        "        avg_rating = aspect_reviews['rating_x'].mean()\n",
        "        avg_compound = aspect_reviews['compound_score'].mean()\n",
        "        \n",
        "        pos_count = sum((aspect_reviews['compound_score'] > 0.05))\n",
        "        neu_count = sum((aspect_reviews['compound_score'] >= -0.05) & (aspect_reviews['compound_score'] <= 0.05))\n",
        "        neg_count = sum((aspect_reviews['compound_score'] < -0.05))\n",
        "        \n",
        "        total_count = pos_count + neu_count + neg_count\n",
        "        \n",
        "        # Find most positive and negative reviews for this aspect\n",
        "        if pos_count > 0:\n",
        "            most_positive = aspect_reviews.loc[aspect_reviews['compound_score'].idxmax()]\n",
        "            pos_review = {\n",
        "                'text': most_positive['review_text'],\n",
        "                'rating': most_positive['rating_x'],\n",
        "                'sentiment': most_positive['compound_score']\n",
        "            }\n",
        "        else:\n",
        "            pos_review = None\n",
        "            \n",
        "        if neg_count > 0:\n",
        "            most_negative = aspect_reviews.loc[aspect_reviews['compound_score'].idxmin()]\n",
        "            neg_review = {\n",
        "                'text': most_negative['review_text'],\n",
        "                'rating': most_negative['rating_x'],\n",
        "                'sentiment': most_negative['compound_score']\n",
        "            }\n",
        "        else:\n",
        "            neg_review = None\n",
        "        \n",
        "        aspect_analysis.append({\n",
        "            'aspect': aspect,\n",
        "            'mention_count': aspect_reviews.shape[0],\n",
        "            'mention_percentage': aspect_reviews.shape[0] / product_data.shape[0] * 100,\n",
        "            'avg_rating': avg_rating,\n",
        "            'avg_sentiment': avg_compound,\n",
        "            'positive_pct': pos_count / total_count * 100 if total_count > 0 else 0,\n",
        "            'neutral_pct': neu_count / total_count * 100 if total_count > 0 else 0,\n",
        "            'negative_pct': neg_count / total_count * 100 if total_count > 0 else 0,\n",
        "            'most_positive_review': pos_review,\n",
        "            'most_negative_review': neg_review\n",
        "        })\n",
        "    \n",
        "    # Sort aspects by mention count\n",
        "    aspect_analysis = sorted(aspect_analysis, key=lambda x: x['mention_count'], reverse=True)\n",
        "    \n",
        "    # Find key strengths and weaknesses\n",
        "    strengths = [a for a in aspect_analysis if a['avg_sentiment'] > 0.1][:3]\n",
        "    weaknesses = [a for a in aspect_analysis if a['avg_sentiment'] < -0.1][:3]\n",
        "    \n",
        "    # Create visualizations\n",
        "    visualizations = {}\n",
        "    \n",
        "    # 1. Aspect sentiment distribution\n",
        "    aspect_df = pd.DataFrame(aspect_analysis)\n",
        "    if not aspect_df.empty:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        aspect_df = aspect_df.sort_values('avg_sentiment', ascending=False)\n",
        "        sns.barplot(x='avg_sentiment', y='aspect', data=aspect_df, palette='RdYlGn')\n",
        "        \n",
        "        plt.axvline(x=0, color='gray', linestyle='--')\n",
        "        plt.title(f'Aspect Sentiment Analysis for {product_info[\"product_name\"]}')\n",
        "        plt.xlabel('Average Sentiment Score')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save figure to buffer\n",
        "        from io import BytesIO\n",
        "        import base64\n",
        "        \n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        visualizations['aspect_sentiment'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "    \n",
        "    # Return complete analysis\n",
        "    return {\n",
        "        'product_info': product_info,\n",
        "        'aspect_analysis': aspect_analysis,\n",
        "        'strengths': strengths,\n",
        "        'weaknesses': weaknesses,\n",
        "        'visualizations': visualizations\n",
        "    }\n",
        "\n",
        "# Test the product analyzer with a sample product\n",
        "sample_product_id = merged_data['product_id'].iloc[0]\n",
        "product_analysis = analyze_product(merged_data, sample_product_id)\n",
        "\n",
        "# Print summary of the analysis\n",
        "print(\"\\nProduct Analysis Summary:\")\n",
        "print(f\"Product: {product_analysis['product_info']['product_name']} by {product_analysis['product_info']['brand_name']}\")\n",
        "print(f\"Average Rating: {product_analysis['product_info']['avg_rating']:.2f}\")\n",
        "print(f\"Review Count: {product_analysis['product_info']['review_count']}\")\n",
        "print(\"\\nAspects Mentioned (Top 3):\")\n",
        "\n",
        "for aspect in product_analysis['aspect_analysis'][:3]:\n",
        "    print(f\"- {aspect['aspect'].capitalize()}: {aspect['mention_count']} mentions, {aspect['avg_sentiment']:.2f} sentiment\")\n",
        "\n",
        "print(\"\\nStrengths:\")\n",
        "for strength in product_analysis['strengths']:\n",
        "    print(f\"- {strength['aspect'].capitalize()}: {strength['avg_sentiment']:.2f} sentiment\")\n",
        "\n",
        "print(\"\\nWeaknesses:\")\n",
        "for weakness in product_analysis['weaknesses']:\n",
        "    print(f\"- {weakness['aspect'].capitalize()}: {weakness['avg_sentiment']:.2f} sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating an Interactive Dashboard Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function would create a more comprehensive dashboard in a notebook\n",
        "def create_dashboard(data, aspect_dict=beauty_aspects):\n",
        "    \"\"\"Create interactive dashboard for ABSA analysis\"\"\"\n",
        "    \n",
        "    # Overview statistics\n",
        "    total_products = data['product_id'].nunique()\n",
        "    total_brands = data['brand_name'].nunique()\n",
        "    total_reviews = data.shape[0]\n",
        "    avg_rating = data['rating'].mean()\n",
        "    \n",
        "    print(\"=== Sephora Product Reviews ABSA Dashboard ===\")\n",
        "    print(f\"Total Products: {total_products}\")\n",
        "    print(f\"Total Brands: {total_brands}\")\n",
        "    print(f\"Total Reviews: {total_reviews}\")\n",
        "    print(f\"Average Rating: {avg_rating:.2f}\")\n",
        "    \n",
        "    # 1. Overall aspect sentiment analysis\n",
        "    print(\"\\n=== Aspect Sentiment Analysis ===\")\n",
        "    \n",
        "    aspect_counts = []\n",
        "    for aspect in aspect_dict.keys():\n",
        "        mention_count = data[data[f'has_{aspect}'] == 1].shape[0]\n",
        "        mention_pct = mention_count / total_reviews * 100\n",
        "        \n",
        "        aspect_data = data[data[f'has_{aspect}'] == 1]\n",
        "        if aspect_data.shape[0] > 0:\n",
        "            avg_sentiment = aspect_data['compound_score'].mean()\n",
        "        else:\n",
        "            avg_sentiment = 0\n",
        "            \n",
        "        aspect_counts.append({\n",
        "            'aspect': aspect,\n",
        "            'mention_count': mention_count,\n",
        "            'mention_pct': mention_pct,\n",
        "            'avg_sentiment': avg_sentiment\n",
        "        })\n",
        "    \n",
        "    aspect_df = pd.DataFrame(aspect_counts)\n",
        "    aspect_df = aspect_df.sort_values('mention_count', ascending=False)\n",
        "    \n",
        "    print(aspect_df)\n",
        "    \n",
        "    # Plot aspect mentions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='mention_count', y='aspect', data=aspect_df)\n",
        "    plt.title('Number of Reviews Mentioning Each Aspect')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot aspect sentiment\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.barplot(x='avg_sentiment', y='aspect', data=aspect_df, palette='RdYlGn')\n",
        "    plt.axvline(x=0, color='gray', linestyle='--')\n",
        "    plt.title('Average Sentiment Score by Aspect')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Brand performance by aspect\n",
        "    print(\"\\n=== Top Brands Performance by Aspect ===\")\n",
        "    \n",
        "    # Get top brands by review count\n",
        "    top_brands = data['brand_name'].value_counts().head(10).index.tolist()\n",
        "    \n",
        "    # Create dataframe with brand-aspect analysis\n",
        "    brand_aspect_df = create_brand_aspect_sentiment(data, top_brands, aspect_dict.keys())\n",
        "    \n",
        "    # Create heatmap of brand-aspect sentiment\n",
        "    brand_aspect_pivot = brand_aspect_df.pivot_table(\n",
        "        index='brand', \n",
        "        columns='aspect', \n",
        "        values='avg_compound'\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(brand_aspect_pivot, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
        "    plt.title('Average Sentiment Score by Brand and Aspect')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. Category performance by aspect\n",
        "    print(\"\\n=== Product Categories Performance by Aspect ===\")\n",
        "    \n",
        "    # Create dataframe with category-aspect analysis\n",
        "    if 'primary_category' in data.columns:\n",
        "        category_aspect_df = analyze_category_aspects(data, 'primary_category', aspect_dict.keys())\n",
        "        \n",
        "        # Create heatmap of category-aspect sentiment\n",
        "        category_aspect_pivot = category_aspect_df.pivot_table(\n",
        "            index='category', \n",
        "            columns='aspect', \n",
        "            values='avg_compound'\n",
        "        )\n",
        "        \n",
        "        plt.figure(figsize=(14, 10))\n",
        "        sns.heatmap(category_aspect_pivot, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
        "        plt.title('Average Sentiment Score by Category and Aspect')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # 4. Top products by aspect\n",
        "    print(\"\\n=== Top Products by Aspect ===\")\n",
        "    \n",
        "    for aspect in aspect_dict.keys():\n",
        "        # Get products with at least 5 mentions of this aspect\n",
        "        aspect_products = data[data[f'has_{aspect}'] == 1].groupby(['product_id', 'product_name', 'brand_name']).agg({\n",
        "            'compound_score': ['mean', 'count'],\n",
        "            'rating': 'mean'\n",
        "        }).reset_index()\n",
        "        \n",
        "        aspect_products.columns = ['product_id', 'product_name', 'brand_name', 'avg_sentiment', 'mention_count', 'avg_rating']\n",
        "        \n",
        "        # Filter products with enough mentions\n",
        "        aspect_products = aspect_products[aspect_products['mention_count'] >= 5]\n",
        "        \n",
        "        if aspect_products.shape[0] > 0:\n",
        "            # Get top 5 by sentiment\n",
        "            top_products = aspect_products.sort_values('avg_sentiment', ascending=False).head(5)\n",
        "            \n",
        "            print(f\"\\nTop 5 Products for {aspect.capitalize()}:\")\n",
        "            print(top_products[['product_name', 'brand_name', 'avg_sentiment', 'mention_count']])\n",
        "    \n",
        "    # 5. Trend analysis for aspects over time\n",
        "    print(\"\\n=== Aspect Sentiment Trends Over Time ===\")\n",
        "    \n",
        "    # Analyze trends for top mentioned aspects\n",
        "    top_aspects = aspect_df.head(3)['aspect'].tolist()\n",
        "    \n",
        "    for aspect in top_aspects:\n",
        "        trend_data = analyze_aspect_trends(data, aspect, 'quarter')\n",
        "        print(f\"\\nTrend for {aspect.capitalize()}:\")\n",
        "        if isinstance(trend_data, pd.DataFrame):\n",
        "            print(trend_data)\n",
        "    \n",
        "    return \"Dashboard completed\"\n",
        "\n",
        "# Test the dashboard function\n",
        "# create_dashboard(merged_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Comprehensive ABSA Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def absa_pipeline(product_info_path, review_files, sample_size=None):\n",
        "    \"\"\"\n",
        "    Complete ABSA pipeline for Sephora product reviews\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    product_info_path : str\n",
        "        Path to the product info CSV file\n",
        "    review_files : list\n",
        "        List of paths to review CSV files\n",
        "    sample_size : int, optional\n",
        "        Number of random samples to take from each review file\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing the processed data and analysis results\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Load data\n",
        "    print(\"Loading data...\")\n",
        "    product_info = pd.read_csv(product_info_path)\n",
        "    \n",
        "    reviews_list = []\n",
        "    for file in review_files:\n",
        "        df = pd.read_csv(file)\n",
        "        if sample_size is not None:\n",
        "            df = df.sample(min(sample_size, df.shape[0]), random_state=42)\n",
        "        reviews_list.append(df)\n",
        "        \n",
        "    reviews = pd.concat(reviews_list, ignore_index=True)\n",
        "    \n",
        "    print(f\"Loaded {product_info.shape[0]} products and {reviews.shape[0]} reviews.\")\n",
        "    \n",
        "    # 2. Merge data\n",
        "    print(\"Merging datasets...\")\n",
        "    merged_data = pd.merge(reviews, product_info, on=['product_id', 'product_name', 'brand_name', 'price_usd'], how='inner')\n",
        "    print(f\"Merged data shape: {merged_data.shape}\")\n",
        "    \n",
        "    # 3. Preprocess text\n",
        "    print(\"Preprocessing text...\")\n",
        "    # Download NLTK resources if not already done\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "    \n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "        \n",
        "    try:\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "        \n",
        "    try:\n",
        "        nltk.data.find('sentiment/vader_lexicon')\n",
        "    except LookupError:\n",
        "        nltk.download('vader_lexicon')\n",
        "    \n",
        "    # Clean text\n",
        "    merged_data['clean_review_text'] = merged_data['review_text'].apply(clean_text)\n",
        "    merged_data['clean_review_title'] = merged_data['review_title'].apply(clean_text)\n",
        "    \n",
        "    # Tokenize\n",
        "    merged_data['tokens'] = merged_data['clean_review_text'].apply(tokenize_and_lemmatize)\n",
        "    merged_data['processed_text'] = merged_data['tokens'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    # 4. Extract aspects\n",
        "    print(\"Extracting aspects...\")\n",
        "    merged_data['aspects'] = merged_data['clean_review_text'].apply(lambda x: extract_aspects(x, beauty_aspects))\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        merged_data[f'has_{aspect}'] = merged_data['aspects'].apply(lambda x: 1 if aspect in x else 0)\n",
        "    \n",
        "    # 5. Perform sentiment analysis\n",
        "    print(\"Performing sentiment analysis...\")\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    merged_data['sentiment_scores'] = merged_data['clean_review_text'].apply(get_sentiment_scores)\n",
        "    \n",
        "    # Extract sentiment scores\n",
        "    merged_data['neg_score'] = merged_data['sentiment_scores'].apply(lambda x: x['neg'])\n",
        "    merged_data['neu_score'] = merged_data['sentiment_scores'].apply(lambda x: x['neu'])\n",
        "    merged_data['pos_score'] = merged_data['sentiment_scores'].apply(lambda x: x['pos'])\n",
        "    merged_data['compound_score'] = merged_data['sentiment_scores'].apply(lambda x: x['compound'])\n",
        "    \n",
        "    # 6. Create sentiment classification\n",
        "    merged_data['sentiment_label'] = merged_data['compound_score'].apply(\n",
        "        lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
        "    )\n",
        "    \n",
        "    # 7. Perform aspect-level sentiment analysis\n",
        "    print(\"Analyzing aspect-level sentiment...\")\n",
        "    aspect_sentiment = []\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        # Filter reviews that mention this aspect\n",
        "        aspect_reviews = merged_data[merged_data[f'has_{aspect}'] == 1]\n",
        "        \n",
        "        if aspect_reviews.shape[0] == 0:\n",
        "            continue\n",
        "        \n",
        "        # Calculate sentiment metrics\n",
        "        avg_rating = aspect_reviews['rating_x'].mean()\n",
        "        avg_compound = aspect_reviews['compound_score'].mean()\n",
        "        \n",
        "        pos_count = sum((aspect_reviews['compound_score'] > 0.05))\n",
        "        neu_count = sum((aspect_reviews['compound_score'] >= -0.05) & (aspect_reviews['compound_score'] <= 0.05))\n",
        "        neg_count = sum((aspect_reviews['compound_score'] < -0.05))\n",
        "        \n",
        "        total_count = pos_count + neu_count + neg_count\n",
        "        \n",
        "        aspect_sentiment.append({\n",
        "            'aspect': aspect,\n",
        "            'avg_rating': avg_rating,\n",
        "            'avg_compound': avg_compound,\n",
        "            'positive_count': pos_count,\n",
        "            'neutral_count': neu_count,\n",
        "            'negative_count': neg_count,\n",
        "            'total_count': total_count,\n",
        "            'positive_pct': pos_count / total_count * 100 if total_count > 0 else 0,\n",
        "            'neutral_pct': neu_count / total_count * 100 if total_count > 0 else 0,\n",
        "            'negative_pct': neg_count / total_count * 100 if total_count > 0 else 0\n",
        "        })\n",
        "    \n",
        "    aspect_sentiment_df = pd.DataFrame(aspect_sentiment)\n",
        "    \n",
        "    # 8. Return the results\n",
        "    print(\"ABSA pipeline completed successfully!\")\n",
        "    return {\n",
        "        'merged_data': merged_data,\n",
        "        'aspect_sentiment': aspect_sentiment_df,\n",
        "        'beauty_aspects': beauty_aspects,\n",
        "        'product_count': merged_data['product_id'].nunique(),\n",
        "        'brand_count': merged_data['brand_name'].nunique(),\n",
        "        'review_count': merged_data.shape[0]\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "# pipeline_results = absa_pipeline(\n",
        "#     'product_info.csv',\n",
        "#     ['review_0-250.csv', 'review_250-500.csv', 'review_500-750.csv', 'review_750-1250.csv', 'review_1250-end.csv'],\n",
        "#     sample_size=1000\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Application Case Studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def case_study_1_brand_comparison(merged_data):\n",
        "#     \"\"\"\n",
        "#     Case Study 1: Brand Comparison\n",
        "    \n",
        "#     Compare two major beauty brands across different aspects to understand \n",
        "#     their competitive advantages and disadvantages.\n",
        "#     \"\"\"\n",
        "#     # Get top brands by review count\n",
        "#     top_brands = merged_data['brand_name'].value_counts().head(10).index.tolist()\n",
        "    \n",
        "#     if len(top_brands) < 2:\n",
        "#         return \"Not enough brands for comparison\"\n",
        "    \n",
        "#     # Select two brands to compare\n",
        "#     brand1 = top_brands[0]\n",
        "#     brand2 = top_brands[1]\n",
        "    \n",
        "#     print(f\"Comparing {brand1} vs {brand2}\")\n",
        "    \n",
        "#     # Filter data for the two brands\n",
        "#     brand1_data = merged_data[merged_data['brand_name'] == brand1]\n",
        "#     brand2_data = merged_data[merged_data['brand_name'] == brand2]\n",
        "    \n",
        "#     # Create comparison dataframe\n",
        "#     comparison = []\n",
        "    \n",
        "#     for aspect in beauty_aspects.keys():\n",
        "#         # Calculate metrics for brand 1\n",
        "#         brand1_aspect = brand1_data[brand1_data[f'has_{aspect}'] == 1]\n",
        "#         if brand1_aspect.shape[0] > 0:\n",
        "#             brand1_sentiment = brand1_aspect['compound_score'].mean()\n",
        "#             brand1_mentions = brand1_aspect.shape[0]\n",
        "#             brand1_mention_pct = brand1_mentions / brand1_data.shape[0] * 100\n",
        "#         else:\n",
        "#             brand1_sentiment = None\n",
        "#             brand1_mentions = 0\n",
        "#             brand1_mention_pct = 0\n",
        "            \n",
        "#         # Calculate metrics for brand 2\n",
        "#         brand2_aspect = brand2_data[brand2_data[f'has_{aspect}'] == 1]\n",
        "#         if brand2_aspect.shape[0] > 0:\n",
        "#             brand2_sentiment = brand2_aspect['compound_score'].mean()\n",
        "#             brand2_mentions = brand2_aspect.shape[0]\n",
        "#             brand2_mention_pct = brand2_mentions / brand2_data.shape[0] * 100\n",
        "#         else:\n",
        "#             brand2_sentiment = None\n",
        "#             brand2_mentions = 0\n",
        "#             brand2_mention_pct = 0\n",
        "            \n",
        "#         # Calculate sentiment difference\n",
        "#         if brand1_sentiment is not None and brand2_sentiment is not None:\n",
        "#             sentiment_diff = brand1_sentiment - brand2_sentiment\n",
        "#         else:\n",
        "#             sentiment_diff = None\n",
        "            \n",
        "#         comparison.append({\n",
        "#             'aspect': aspect,\n",
        "#             f'{brand1}_sentiment': brand1_sentiment,\n",
        "#             f'{brand1}_mentions': brand1_mentions,\n",
        "#             f'{brand1}_mention_pct': brand1_mention_pct,\n",
        "#             f'{brand2}_sentiment': brand2_sentiment,\n",
        "#             f'{brand2}_mentions': brand2_mentions,\n",
        "#             f'{brand2}_mention_pct': brand2_mention_pct,\n",
        "#             'sentiment_diff': sentiment_diff\n",
        "#         })\n",
        "    \n",
        "#     comparison_df = pd.DataFrame(comparison)\n",
        "    \n",
        "#     # Filter to aspects with enough mentions\n",
        "#     valid_comparison = comparison_df[(comparison_df[f'{brand1}_mentions'] >= 5) & \n",
        "#                                     (comparison_df[f'{brand2}_mentions'] >= 5)].copy()\n",
        "    \n",
        "#     if valid_comparison.shape[0] == 0:\n",
        "#         return \"Not enough shared aspects for comparison\"\n",
        "    \n",
        "#     # Sort by absolute sentiment difference\n",
        "#     valid_comparison['abs_diff'] = valid_comparison['sentiment_diff'].abs()\n",
        "#     valid_comparison = valid_comparison.sort_values('abs_diff', ascending=False)\n",
        "    \n",
        "#     # Visualize the comparison\n",
        "#     plt.figure(figsize=(14, 8))\n",
        "    \n",
        "#     aspects = valid_comparison['aspect'].tolist()\n",
        "#     brand1_sentiments = valid_comparison[f'{brand1}_sentiment'].tolist()\n",
        "#     brand2_sentiments = valid_comparison[f'{brand2}_sentiment'].tolist()\n",
        "    \n",
        "#     x = np.arange(len(aspects))\n",
        "#     width = 0.35\n",
        "    \n",
        "#     fig, ax = plt.subplots(figsize=(14, 8))\n",
        "#     rects1 = ax.bar(x - width/2, brand1_sentiments, width, label=brand1, color='skyblue')\n",
        "#     rects2 = ax.bar(x + width/2, brand2_sentiments, width, label=brand2, color='lightcoral')\n",
        "    \n",
        "#     ax.set_ylabel('Sentiment Score')\n",
        "#     ax.set_title(f'Aspect Sentiment Comparison: {brand1} vs {brand2}')\n",
        "#     ax.set_xticks(x)\n",
        "#     ax.set_xticklabels([a.capitalize() for a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def case_study_1_brand_comparison(merged_data):\n",
        "    \"\"\"\n",
        "    Case Study 1: Brand Comparison\n",
        "    \n",
        "    Compare two major beauty brands across different aspects to understand \n",
        "    their competitive advantages and disadvantages.\n",
        "    \"\"\"\n",
        "    # Get top brands by review count\n",
        "    top_brands = merged_data['brand_name'].value_counts().head(10).index.tolist()\n",
        "    \n",
        "    if len(top_brands) < 2:\n",
        "        return \"Not enough brands for comparison\"\n",
        "    \n",
        "    # Select two brands to compare\n",
        "    brand1 = top_brands[0]\n",
        "    brand2 = top_brands[1]\n",
        "    \n",
        "    print(f\"Comparing {brand1} vs {brand2}\")\n",
        "    \n",
        "    # Filter data for the two brands\n",
        "    brand1_data = merged_data[merged_data['brand_name'] == brand1]\n",
        "    brand2_data = merged_data[merged_data['brand_name'] == brand2]\n",
        "    \n",
        "    # Create comparison dataframe\n",
        "    comparison = []\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        # Calculate metrics for brand 1\n",
        "        brand1_aspect = brand1_data[brand1_data[f'has_{aspect}'] == 1]\n",
        "        if brand1_aspect.shape[0] > 0:\n",
        "            brand1_sentiment = brand1_aspect['compound_score'].mean()\n",
        "            brand1_mentions = brand1_aspect.shape[0]\n",
        "            brand1_mention_pct = brand1_mentions / brand1_data.shape[0] * 100\n",
        "        else:\n",
        "            brand1_sentiment = None\n",
        "            brand1_mentions = 0\n",
        "            brand1_mention_pct = 0\n",
        "            \n",
        "        # Calculate metrics for brand 2\n",
        "        brand2_aspect = brand2_data[brand2_data[f'has_{aspect}'] == 1]\n",
        "        if brand2_aspect.shape[0] > 0:\n",
        "            brand2_sentiment = brand2_aspect['compound_score'].mean()\n",
        "            brand2_mentions = brand2_aspect.shape[0]\n",
        "            brand2_mention_pct = brand2_mentions / brand2_data.shape[0] * 100\n",
        "        else:\n",
        "            brand2_sentiment = None\n",
        "            brand2_mentions = 0\n",
        "            brand2_mention_pct = 0\n",
        "            \n",
        "        # Calculate sentiment difference\n",
        "        if brand1_sentiment is not None and brand2_sentiment is not None:\n",
        "            sentiment_diff = brand1_sentiment - brand2_sentiment\n",
        "        else:\n",
        "            sentiment_diff = None\n",
        "            \n",
        "        comparison.append({\n",
        "            'aspect': aspect,\n",
        "            f'{brand1}_sentiment': brand1_sentiment,\n",
        "            f'{brand1}_mentions': brand1_mentions,\n",
        "            f'{brand1}_mention_pct': brand1_mention_pct,\n",
        "            f'{brand2}_sentiment': brand2_sentiment,\n",
        "            f'{brand2}_mentions': brand2_mentions,\n",
        "            f'{brand2}_mention_pct': brand2_mention_pct,\n",
        "            'sentiment_diff': sentiment_diff\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison)\n",
        "    \n",
        "    # Filter to aspects with enough mentions\n",
        "    valid_comparison = comparison_df[(comparison_df[f'{brand1}_mentions'] >= 5) & \n",
        "                                    (comparison_df[f'{brand2}_mentions'] >= 5)].copy()\n",
        "    \n",
        "    if valid_comparison.shape[0] == 0:\n",
        "        return \"Not enough shared aspects for comparison\"\n",
        "    \n",
        "    # Sort by absolute sentiment difference\n",
        "    valid_comparison['abs_diff'] = valid_comparison['sentiment_diff'].abs()\n",
        "    valid_comparison = valid_comparison.sort_values('abs_diff', ascending=False)\n",
        "    \n",
        "    # Visualize the comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    \n",
        "    aspects = valid_comparison['aspect'].tolist()\n",
        "    brand1_sentiments = valid_comparison[f'{brand1}_sentiment'].tolist()\n",
        "    brand2_sentiments = valid_comparison[f'{brand2}_sentiment'].tolist()\n",
        "    \n",
        "    x = np.arange(len(aspects))\n",
        "    width = 0.35\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    rects1 = ax.bar(x - width/2, brand1_sentiments, width, label=brand1, color='skyblue')\n",
        "    rects2 = ax.bar(x + width/2, brand2_sentiments, width, label=brand2, color='lightcoral')\n",
        "    \n",
        "    ax.set_ylabel('Sentiment Score')\n",
        "    ax.set_title(f'Aspect Sentiment Comparison: {brand1} vs {brand2}')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([a.capitalize() for a in aspects], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.2f}',\n",
        "                       xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                       xytext=(0, 3),  # 3 points vertical offset\n",
        "                       textcoords=\"offset points\",\n",
        "                       ha='center', va='bottom')\n",
        "    \n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Generate insights\n",
        "    print(\"\\nKey Insights:\")\n",
        "    for _, row in valid_comparison.head(3).iterrows():\n",
        "        aspect = row['aspect'].capitalize()\n",
        "        if row['sentiment_diff'] > 0:\n",
        "            winner = brand1\n",
        "            loser = brand2\n",
        "            diff = row['sentiment_diff']\n",
        "        else:\n",
        "            winner = brand2\n",
        "            loser = brand1\n",
        "            diff = -row['sentiment_diff']\n",
        "            \n",
        "        print(f\"- {aspect}: {winner} outperforms {loser} by {diff:.2f} sentiment points\")\n",
        "        \n",
        "    return valid_comparison\n",
        "\n",
        "\n",
        "# case_study_1_brand_comparison(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def case_study_2_sentiment_drivers(merged_data, product_id):\n",
        "    \"\"\"\n",
        "    Case Study 2: Product Sentiment Drivers\n",
        "    \n",
        "    Analyze what aspects are driving positive or negative sentiment\n",
        "    for a specific product, to identify improvement opportunities.\n",
        "    \"\"\"\n",
        "    # Filter data for the specific product\n",
        "    product_data = merged_data[merged_data['product_id'] == product_id]\n",
        "    \n",
        "    if product_data.shape[0] == 0:\n",
        "        return \"Product not found in dataset\"\n",
        "        \n",
        "    product_name = product_data['product_name'].iloc[0]\n",
        "    brand_name = product_data['brand_name'].iloc[0]\n",
        "    \n",
        "    print(f\"Analyzing sentiment drivers for {brand_name} - {product_name}\")\n",
        "    print(f\"Total reviews: {product_data.shape[0]}\")\n",
        "    \n",
        "    # Create a dataframe of aspect sentiments\n",
        "    aspect_sentiments = []\n",
        "    \n",
        "    for aspect in beauty_aspects.keys():\n",
        "        aspect_data = product_data[product_data[f'has_{aspect}'] == 1]\n",
        "        if aspect_data.shape[0] > 0:\n",
        "            sentiment = aspect_data['compound_score'].mean()\n",
        "            positive = aspect_data[aspect_data['compound_score'] > 0.05].shape[0]\n",
        "            negative = aspect_data[aspect_data['compound_score'] < -0.05].shape[0]\n",
        "            neutral = aspect_data.shape[0] - positive - negative\n",
        "            \n",
        "            aspect_sentiments.append({\n",
        "                'aspect': aspect,\n",
        "                'mentions': aspect_data.shape[0],\n",
        "                'mention_pct': aspect_data.shape[0] / product_data.shape[0] * 100,\n",
        "                'sentiment': sentiment,\n",
        "                'positive': positive,\n",
        "                'negative': negative,\n",
        "                'neutral': neutral,\n",
        "                'pos_pct': positive / aspect_data.shape[0] * 100 if aspect_data.shape[0] > 0 else 0,\n",
        "                'neg_pct': negative / aspect_data.shape[0] * 100 if aspect_data.shape[0] > 0 else 0\n",
        "            })\n",
        "    \n",
        "    aspect_df = pd.DataFrame(aspect_sentiments)\n",
        "    \n",
        "    # Filter to aspects with enough mentions\n",
        "    valid_aspects = aspect_df[aspect_df['mentions'] >= 3].copy()\n",
        "    \n",
        "    if valid_aspects.shape[0] == 0:\n",
        "        return \"Not enough aspect mentions for analysis\"\n",
        "    \n",
        "    # Sort by mentions (most discussed aspects first)\n",
        "    valid_aspects = valid_aspects.sort_values('mentions', ascending=False)\n",
        "    \n",
        "    # Visualize the sentiment drivers\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Create horizontal bar chart of aspect sentiments\n",
        "    aspects = valid_aspects['aspect'].tolist()\n",
        "    sentiments = valid_aspects['sentiment'].tolist()\n",
        "    mentions = valid_aspects['mentions'].tolist()\n",
        "    \n",
        "    # Normalize mentions for visualization\n",
        "    max_mentions = max(mentions)\n",
        "    norm_mentions = [m/max_mentions * 100 for m in mentions]\n",
        "    \n",
        "    # Create colormap based on sentiment\n",
        "    colors = ['red' if s < -0.05 else 'green' if s > 0.05 else 'gray' for s in sentiments]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    bars = ax.barh(aspects, sentiments, color=colors, alpha=0.7)\n",
        "    \n",
        "    # Add bar width based on mention count\n",
        "    for i, bar in enumerate(bars):\n",
        "        bar.set_alpha(0.4 + (norm_mentions[i] / 200))  # Scale alpha between 0.4 and 0.9\n",
        "        bar.set_height(0.5)  # Set consistent height\n",
        "    \n",
        "    # Add mention count annotation\n",
        "    for i, aspect in enumerate(aspects):\n",
        "        ax.text(0, i, f\" {mentions[i]} mentions\", va='center', fontsize=10)\n",
        "    \n",
        "    # Add neutral line and formatting\n",
        "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    ax.set_xlabel('Sentiment Score')\n",
        "    ax.set_title(f'Aspect Sentiment Drivers for {brand_name} - {product_name}')\n",
        "    ax.set_xlim(-1, 1)\n",
        "    \n",
        "    # Sort aspects for better visualization\n",
        "    sorted_indices = np.argsort(sentiments)\n",
        "    ax.set_yticks(range(len(aspects)))\n",
        "    ax.set_yticklabels([aspects[i].capitalize() for i in sorted_indices])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Generate insights\n",
        "    print(\"\\nKey Insights:\")\n",
        "    \n",
        "    # Most positive aspects\n",
        "    top_positive = valid_aspects.sort_values('sentiment', ascending=False).head(2)\n",
        "    print(\"Strengths:\")\n",
        "    for _, row in top_positive.iterrows():\n",
        "        print(f\"- {row['aspect'].capitalize()}: {row['sentiment']:.2f} sentiment, mentioned in {row['mention_pct']:.1f}% of reviews\")\n",
        "    \n",
        "    # Most negative aspects\n",
        "    top_negative = valid_aspects.sort_values('sentiment').head(2)\n",
        "    print(\"\\nImprovement Areas:\")\n",
        "    for _, row in top_negative.iterrows():\n",
        "        print(f\"- {row['aspect'].capitalize()}: {row['sentiment']:.2f} sentiment, mentioned in {row['mention_pct']:.1f}% of reviews\")\n",
        "    \n",
        "    # Most mentioned aspects\n",
        "    top_mentioned = valid_aspects.sort_values('mentions', ascending=False).head(1)\n",
        "    print(f\"\\nMost Discussed Feature: {top_mentioned['aspect'].iloc[0].capitalize()} - \" +\n",
        "          f\"mentioned in {top_mentioned['mention_pct'].iloc[0]:.1f}% of reviews\")\n",
        "    \n",
        "    return valid_aspects\n",
        "\n",
        "\n",
        "# case_study_2_sentiment_drivers(merged_data, sample_product_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def case_study_3_competitive_landscape(merged_data, aspect_of_interest):\n",
        "    \"\"\"\n",
        "    Case Study 3: Competitive Landscape Analysis\n",
        "    \n",
        "    Compare multiple brands on a specific aspect to identify\n",
        "    market leaders and laggards for that product feature.\n",
        "    \"\"\"\n",
        "    # Check if aspect exists in our aspect list\n",
        "    if f'has_{aspect_of_interest}' not in merged_data.columns:\n",
        "        return f\"Aspect '{aspect_of_interest}' not found in dataset\"\n",
        "    \n",
        "    print(f\"Analyzing competitive landscape for aspect: {aspect_of_interest}\")\n",
        "    \n",
        "    # Filter data to reviews that mention the aspect\n",
        "    aspect_data = merged_data[merged_data[f'has_{aspect_of_interest}'] == 1]\n",
        "    \n",
        "    if aspect_data.shape[0] < 10:\n",
        "        return f\"Not enough mentions of '{aspect_of_interest}' for analysis\"\n",
        "    \n",
        "    # Get top brands by review count\n",
        "    brand_counts = aspect_data['brand_name'].value_counts()\n",
        "    top_brands = brand_counts[brand_counts >= 5].index.tolist()\n",
        "    \n",
        "    if len(top_brands) < 3:\n",
        "        return \"Not enough brands with sufficient mentions for comparison\"\n",
        "    \n",
        "    # Calculate brand metrics for this aspect\n",
        "    brand_metrics = []\n",
        "    \n",
        "    for brand in top_brands:\n",
        "        brand_aspect_data = aspect_data[aspect_data['brand_name'] == brand]\n",
        "        total_brand_reviews = merged_data[merged_data['brand_name'] == brand].shape[0]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        sentiment = brand_aspect_data['compound_score'].mean()\n",
        "        mentions = brand_aspect_data.shape[0]\n",
        "        mention_pct = mentions / total_brand_reviews * 100\n",
        "        \n",
        "        # Calculate sentiment distribution\n",
        "        positive = brand_aspect_data[brand_aspect_data['compound_score'] > 0.05].shape[0]\n",
        "        negative = brand_aspect_data[brand_aspect_data['compound_score'] < -0.05].shape[0]\n",
        "        neutral = mentions - positive - negative\n",
        "        \n",
        "        pos_pct = positive / mentions * 100 if mentions > 0 else 0\n",
        "        neg_pct = negative / mentions * 100 if mentions > 0 else 0\n",
        "        \n",
        "        brand_metrics.append({\n",
        "            'brand': brand,\n",
        "            'sentiment': sentiment,\n",
        "            'mentions': mentions,\n",
        "            'mention_pct': mention_pct,\n",
        "            'positive': positive,\n",
        "            'negative': negative,\n",
        "            'neutral': neutral,\n",
        "            'pos_pct': pos_pct,\n",
        "            'neg_pct': neg_pct\n",
        "        })\n",
        "    \n",
        "    brand_df = pd.DataFrame(brand_metrics)\n",
        "    \n",
        "    # Sort by sentiment (best performing brands first)\n",
        "    brand_df = brand_df.sort_values('sentiment', ascending=False)\n",
        "    \n",
        "    # Visualize the competitive landscape\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    \n",
        "    # Create scatter plot of brands\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Scatter plot with mention % on x-axis and sentiment on y-axis\n",
        "    scatter = ax.scatter(brand_df['mention_pct'], \n",
        "                         brand_df['sentiment'],\n",
        "                         s=brand_df['mentions'] * 5,  # Size based on mention count\n",
        "                         alpha=0.6,\n",
        "                         c=brand_df['pos_pct'],  # Color based on % positive\n",
        "                         cmap='RdYlGn')\n",
        "    \n",
        "    # Add brand labels\n",
        "    for i, row in brand_df.iterrows():\n",
        "        ax.annotate(row['brand'], \n",
        "                   (row['mention_pct'], row['sentiment']),\n",
        "                   xytext=(5, 5),\n",
        "                   textcoords='offset points')\n",
        "    \n",
        "    # Add formatting\n",
        "    ax.set_xlabel(f'% of Reviews Mentioning {aspect_of_interest.capitalize()}')\n",
        "    ax.set_ylabel('Average Sentiment Score')\n",
        "    ax.set_title(f'Competitive Landscape: {aspect_of_interest.capitalize()} Performance')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('% Positive Reviews')\n",
        "    \n",
        "    # Add quadrant lines\n",
        "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
        "    ax.axvline(x=brand_df['mention_pct'].mean(), color='gray', linestyle='--', alpha=0.3)\n",
        "    \n",
        "    # Add quadrant labels\n",
        "    plt.text(brand_df['mention_pct'].max() * 0.9, 0.8, 'High Sentiment\\nHigh Mention',\n",
        "            ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.text(brand_df['mention_pct'].min() * 1.1, 0.8, 'High Sentiment\\nLow Mention',\n",
        "            ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.text(brand_df['mention_pct'].max() * 0.9, -0.8, 'Low Sentiment\\nHigh Mention',\n",
        "            ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.text(brand_df['mention_pct'].min() * 1.1, -0.8, 'Low Sentiment\\nLow Mention',\n",
        "            ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Generate insights\n",
        "    print(\"\\nKey Insights:\")\n",
        "    \n",
        "    # Top performer\n",
        "    top_brand = brand_df.iloc[0]\n",
        "    print(f\"Market Leader: {top_brand['brand']} leads in {aspect_of_interest} with \" +\n",
        "          f\"{top_brand['sentiment']:.2f} sentiment and {top_brand['pos_pct']:.1f}% positive reviews\")\n",
        "    \n",
        "    # Most mentioned\n",
        "    most_mentioned = brand_df.sort_values('mentions', ascending=False).iloc[0]\n",
        "    print(f\"Most Discussed: {most_mentioned['brand']} has the most {aspect_of_interest} mentions \" +\n",
        "          f\"({most_mentioned['mentions']}) with {most_mentioned['sentiment']:.2f} sentiment\")\n",
        "    \n",
        "    # Worst performer with significant mentions\n",
        "    significant_mentions = brand_df[brand_df['mentions'] >= 10]\n",
        "    if not significant_mentions.empty:\n",
        "        worst = significant_mentions.sort_values('sentiment').iloc[0]\n",
        "        print(f\"Improvement Needed: {worst['brand']} has the lowest {aspect_of_interest} sentiment \" +\n",
        "              f\"({worst['sentiment']:.2f}) among brands with significant mentions\")\n",
        "    \n",
        "    return brand_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example function to run all case studies\n",
        "def run_absa_case_studies(merged_data):\n",
        "    \"\"\"\n",
        "    Run all three case studies and generate comprehensive insights\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"CASE STUDY 1: BRAND COMPARISON\")\n",
        "    print(\"=\" * 50)\n",
        "    brand_comparison = case_study_1_brand_comparison(merged_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"CASE STUDY 2: PRODUCT SENTIMENT DRIVERS\")\n",
        "    print(\"=\" * 50)\n",
        "    # Choose a popular product\n",
        "    popular_product = merged_data['product_id'].value_counts().index[0]\n",
        "    product_drivers = case_study_2_sentiment_drivers(merged_data, popular_product)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"CASE STUDY 3: COMPETITIVE LANDSCAPE\")\n",
        "    print(\"=\" * 50)\n",
        "    # Choose a commonly discussed aspect\n",
        "    aspect_counts = {aspect: merged_data[f'has_{aspect}'].sum() \n",
        "                    for aspect in beauty_aspects.keys()}\n",
        "    top_aspect = max(aspect_counts.items(), key=lambda x: x[1])[0]\n",
        "    competitive_landscape = case_study_3_competitive_landscape(merged_data, top_aspect)\n",
        "    \n",
        "    return {\n",
        "        \"brand_comparison\": brand_comparison,\n",
        "        \"product_drivers\": product_drivers,\n",
        "        \"competitive_landscape\": competitive_landscape\n",
        "    }\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "run_absa_case_studies(merged_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSxFEO3mDAfU8fSiN5wqPA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.13.2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
